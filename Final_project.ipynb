{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrAwesomeJr/comp-451-cat-bounding-box/blob/main/Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone"
      ],
      "metadata": {
        "id": "m8snI-1iffqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkRvEaYHfFNU"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import numpy as np\n",
        "\n",
        "#LOADING DATA\n",
        "dataset = fo.zoo.load_zoo_dataset(\n",
        "              \"open-images-v7\",\n",
        "              split=\"train\",\n",
        "              label_types=[\"detections\"],\n",
        "              classes=[\"Cat\"],\n",
        "            #   max_samples=10,\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import cv2"
      ],
      "metadata": {
        "id": "qFuhvsxQfJ6c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batching is not necessary if we're using fasterrcnn_resnet50_fpn.\n",
        "dataset = dataset[:500]\n",
        "batch_size = 100\n",
        "n_samples = len(dataset)\n",
        "x_size = 267\n",
        "y_size = 326"
      ],
      "metadata": {
        "id": "MbVgndlrf-mU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unbatched_data = []\n",
        "unbatched_boxes = []\n",
        "\n",
        "for sample in dataset:\n",
        "    # Load and resize the image\n",
        "    image = cv2.imread(sample['filepath'])\n",
        "    resized_image = cv2.resize(image, (x_size, y_size))\n",
        "\n",
        "    # Convert to tensor\n",
        "    tensor_image = transforms.ToTensor()(resized_image)\n",
        "    unbatched_data.append(tensor_image)\n",
        "\n",
        "    # Extract bounding box for the 'Cat' label\n",
        "    for detection in sample['ground_truth']['detections']:\n",
        "        if detection['label'] == 'Cat':\n",
        "            unbatched_boxes.append(detection['bounding_box'])\n",
        "            break\n",
        "\n",
        "targets = []\n",
        "\n",
        "for i, box in enumerate(unbatched_boxes):\n",
        "    #normalized coordinates to pixel values\n",
        "    x_min, y_min, width, height = box\n",
        "    x_max = x_min + width\n",
        "    y_max = y_min + height\n",
        "\n",
        "    #target dictionary\n",
        "    target = {\n",
        "        'boxes': torch.tensor([[x_min * x_size, y_min * y_size, x_max * x_size, y_max * y_size]], dtype=torch.float32),\n",
        "        'labels': torch.tensor([17], dtype=torch.int64),  #17 is the label for 'Cat'\n",
        "    }\n",
        "    targets.append(target)"
      ],
      "metadata": {
        "id": "fKyNyubmfMVL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unbatched data length: {len(unbatched_data)}\")\n",
        "print(f\"Tensor shape (example): {unbatched_data[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6VtdNtJ4-tE",
        "outputId": "95a0ba7c-d308-4ae1-d41f-3784b9bc9566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unbatched data length: 500\n",
            "Tensor shape (example): torch.Size([3, 326, 267])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING\n",
        "import ssl\n",
        "ssl._create_default_https_context=ssl._create_unverified_context\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None, num_classes=2)\n",
        "output=model(unbatched_data,targets)\n"
      ],
      "metadata": {
        "id": "MRHaqH0qLIiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL EVALUATION\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "batch_size = 20  #processing in batches of 20 images at a time\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(unbatched_data), batch_size):\n",
        "    batch = unbatched_data[i:i + batch_size]\n",
        "    print(i)\n",
        "    # Run inference on the batch\n",
        "    with torch.no_grad():\n",
        "        predictions_batch = model(batch)\n",
        "\n",
        "    predictions.extend(predictions_batch)\n",
        "# Ensure inputs are lists\n",
        "#with torch.no_grad():\n",
        "   # predictions = model(unbatched_data)  # Only pass images for inference\n",
        "\n",
        "# Print predictions\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Image {i} Predictions:\")\n",
        "    print(prediction)"
      ],
      "metadata": {
        "id": "LtZr-BAafPui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#METRICS\n",
        "from sklearn.metrics import average_precision_score\n",
        "test_set = fo.zoo.load_zoo_dataset(\n",
        "              \"open-images-v7\",\n",
        "              split=\"test\",\n",
        "              label_types=[\"detections\"],\n",
        "              classes=[\"Cat\"],\n",
        "            #   max_samples=10,\n",
        "          )\n",
        "test_set = test_set[:500]\n",
        "acc_score = accuracy_score(test_set, predictions)\n",
        "print(f\"Accuracy Score: {acc_score}\")\n",
        "avg_prec_score = average_precision_score(test_set, predictions)\n",
        "print(f\"Average Precision Score: {avg_prec_score}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ3vHZLPLKgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}